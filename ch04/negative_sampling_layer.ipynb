{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.np import *  # import numpy as np\n",
        "from common.layers import Embedding, SigmoidWithLoss\n",
        "import collections\n",
        "\n",
        "\n",
        "class EmbeddingDot:\n",
        "    def __init__(self, W):\n",
        "        self.embed = Embedding(W)\n",
        "        self.params = self.embed.params\n",
        "        self.grads = self.embed.grads\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, h, idx):\n",
        "        target_W = self.embed.forward(idx)\n",
        "        out = np.sum(target_W * h, axis=1)\n",
        "\n",
        "        self.cache = (h, target_W)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        h, target_W = self.cache\n",
        "        dout = dout.reshape(dout.shape[0], 1)\n",
        "\n",
        "        dtarget_W = dout * h\n",
        "        self.embed.backward(dtarget_W)\n",
        "        dh = dout * target_W\n",
        "        return dh\n",
        "\n",
        "\n",
        "class UnigramSampler:\n",
        "    def __init__(self, corpus, power, sample_size):\n",
        "        self.sample_size = sample_size\n",
        "        self.vocab_size = None\n",
        "        self.word_p = None\n",
        "\n",
        "        counts = collections.Counter()\n",
        "        for word_id in corpus:\n",
        "            counts[word_id] += 1\n",
        "\n",
        "        vocab_size = len(counts)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.word_p = np.zeros(vocab_size)\n",
        "        for i in range(vocab_size):\n",
        "            self.word_p[i] = counts[i]\n",
        "\n",
        "        self.word_p = np.power(self.word_p, power)\n",
        "        self.word_p /= np.sum(self.word_p)\n",
        "\n",
        "    def get_negative_sample(self, target):\n",
        "        batch_size = target.shape[0]\n",
        "\n",
        "        if not GPU:\n",
        "            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                p = self.word_p.copy()\n",
        "                target_idx = target[i]\n",
        "                p[target_idx] = 0\n",
        "                p /= p.sum()\n",
        "                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
        "        else:\n",
        "            # GPU(cupy\uff09\ub85c \uacc4\uc0b0\ud560 \ub54c\ub294 \uc18d\ub3c4\ub97c \uc6b0\uc120\ud55c\ub2e4.\n",
        "            # \ubd80\uc815\uc801 \uc608\uc5d0 \ud0c0\uae43\uc774 \ud3ec\ud568\ub420 \uc218 \uc788\ub2e4.\n",
        "            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n",
        "                                               replace=True, p=self.word_p)\n",
        "\n",
        "        return negative_sample\n",
        "\n",
        "\n",
        "class NegativeSamplingLoss:\n",
        "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
        "        self.sample_size = sample_size\n",
        "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
        "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
        "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.embed_dot_layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def forward(self, h, target):\n",
        "        batch_size = target.shape[0]\n",
        "        negative_sample = self.sampler.get_negative_sample(target)\n",
        "\n",
        "        # \uae0d\uc815\uc801 \uc608 \uc21c\uc804\ud30c\n",
        "        score = self.embed_dot_layers[0].forward(h, target)\n",
        "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
        "        loss = self.loss_layers[0].forward(score, correct_label)\n",
        "\n",
        "        # \ubd80\uc815\uc801 \uc608 \uc21c\uc804\ud30c\n",
        "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
        "        for i in range(self.sample_size):\n",
        "            negative_target = negative_sample[:, i]\n",
        "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
        "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dh = 0\n",
        "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
        "            dscore = l0.backward(dout)\n",
        "            dh += l1.backward(dscore)\n",
        "\n",
        "        return dh\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}