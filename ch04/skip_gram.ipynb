{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')  # \ubd80\ubaa8 \ub514\ub809\ud130\ub9ac\uc758 \ud30c\uc77c\uc744 \uac00\uc838\uc62c \uc218 \uc788\ub3c4\ub85d \uc124\uc815\n",
        "from common.layers import *\n",
        "from ch04.negative_sampling_layer import NegativeSamplingLoss\n",
        "\n",
        "\n",
        "class SkipGram:\n",
        "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
        "        V, H = vocab_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        # \uac00\uc911\uce58 \ucd08\uae30\ud654\n",
        "        W_in = 0.01 * rn(V, H).astype('f')\n",
        "        W_out = 0.01 * rn(V, H).astype('f')\n",
        "\n",
        "        # \uacc4\uce35 \uc0dd\uc131\n",
        "        self.in_layer = Embedding(W_in)\n",
        "        self.loss_layers = []\n",
        "        for i in range(2 * window_size):\n",
        "            layer = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
        "            self.loss_layers.append(layer)\n",
        "\n",
        "        # \ubaa8\ub4e0 \uac00\uc911\uce58\uc640 \uae30\uc6b8\uae30\ub97c \ub9ac\uc2a4\ud2b8\uc5d0 \ubaa8\uc740\ub2e4.\n",
        "        layers = [self.in_layer] + self.loss_layers\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "        # \uc778\uc2a4\ud134\uc2a4 \ubcc0\uc218\uc5d0 \ub2e8\uc5b4\uc758 \ubd84\uc0b0 \ud45c\ud604\uc744 \uc800\uc7a5\ud55c\ub2e4.\n",
        "        self.word_vecs = W_in\n",
        "\n",
        "    def forward(self, contexts, target):\n",
        "        h = self.in_layer.forward(target)\n",
        "\n",
        "        loss = 0\n",
        "        for i, layer in enumerate(self.loss_layers):\n",
        "            loss += layer.forward(h, contexts[:, i])\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dh = 0\n",
        "        for i, layer in enumerate(self.loss_layers):\n",
        "            dh += layer.backward(dout)\n",
        "        self.in_layer.backward(dh)\n",
        "        return None\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}